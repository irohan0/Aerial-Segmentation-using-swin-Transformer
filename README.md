# Aerial-Segmentation-using-swin-Transformer
In this project we are going to create a Unet with Swin Transformer architecture and compare it with other models such as Unet, segnet and Deeplabv3+.

## Dataset - https://www.kaggle.com/datasets/humansintheloop/semantic-segmentation-of-aerial-imagery

**Abstract - ** . Rapid urbanization and climate change has significantly impacted the geography of our cities and our environment; thus, necessitating the need for the development of methods to quantify these effects. Aerial imagining and computer vision methods can aid in assessing and quantifying the impact of developments and can also provide valuable insights for urban planning or disaster management. Over the last few years, the deep learning-based segmentation architectures have outperformed the conventional computer vision-based methods for the accurate and precise segmentation of complex scenes, like aerial images, autonomous driving, etc. In the proposed work, a hybrid UNet is proposed, wherein the convolutional backbone of the standard UNet has been replaced by the swim transformer. The introduction of the swim transformer aids in capturing the global details using the self-attention mechanisms; as well as, the model leverages the localization capabilities of the standard UNet for producing the precise segmentation masks. The proposed hybrid UNet is evaluated for the multi-class segmentation of the satellite images captured by the Mohammed Bin Rashid Space Center, Dubai. The paper also presents a comparative analysis of the proposed model with respect to the classical segmentation models like UNet, DeepLab-v3+, and SegNet. The results obtained showcase the superiority of the proposed model. 

## Architecture -
![image](https://github.com/user-attachments/assets/d3877958-3b4f-4dfd-a7f4-893e28d3db1a)

![image](https://github.com/user-attachments/assets/e6ce173f-0bfa-4b4d-90b4-05846ad5dd76)

## Results - 

![image](https://github.com/user-attachments/assets/68b4c632-c48e-429a-bbe0-59152e85613e)

![image](https://github.com/user-attachments/assets/ed46764c-9b3a-4982-8fb6-6890d65bb0b0)

![image](https://github.com/user-attachments/assets/8c41e515-ab48-4877-9508-8763f20e3132)

![image](https://github.com/user-attachments/assets/d167d1f4-05ee-4586-acbc-079df1d41409)


![image](https://github.com/user-attachments/assets/43b63e54-ffe1-42bc-972b-ae6a2a98b403)
